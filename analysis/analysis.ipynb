{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb25f99-6f75-4079-84e9-71f523790375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytrec_eval\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from pyserini.analysis import Analyzer, get_lucene_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fb64a-c4fb-4ad6-a5d0-a7242b76c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/path/to/BEIR/datasets\"\n",
    "dataset = \"dbpedia-entity\"\n",
    "model_path=\"/path/to/dense/model\"\n",
    "func_name=\"maxsim_bm25_qtf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186f51a-f858-422d-96f7-5c461258e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(root_dir, dataset)\n",
    "corpus, queries, qrels = GenericDataLoader(data_path).load(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ae5bf-5e0e-4b84-b413-78c3d186b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(corpus, tokenizer):\n",
    "    sep = \" \"\n",
    "    doc_lens = {}\n",
    "    df = Counter()\n",
    "    d_tf = {}\n",
    "\n",
    "    for cid in tqdm(corpus.keys()):\n",
    "        text = corpus[cid][\"title\"] + sep + corpus[cid][\"text\"]\n",
    "        input_ids = tokenizer(text)\n",
    "        doc_lens[cid] = (len(input_ids))\n",
    "        df.update(list(set(input_ids)))\n",
    "        tf_d = Counter(input_ids)\n",
    "        doc_lens[cid] =len(input_ids)\n",
    "        d_tf[cid] = tf_d\n",
    "        \n",
    "    idf = defaultdict(float)\n",
    "    N = len(corpus)\n",
    "    for w, v in df.items():\n",
    "        idf[w] = np.log(N / v)\n",
    "\n",
    "    doc_len_ave = np.mean(list(doc_lens.values()))\n",
    "    \n",
    "    del df\n",
    "        \n",
    "    return d_tf, idf, doc_lens, doc_len_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc163e-b082-42a2-bcfc-81fe650405c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_d_bm25(d_tf, idf, doc_lens, doc_len_ave):\n",
    "    k1 = 0.9\n",
    "    b = 0.6\n",
    "    token_bm25_score = {}\n",
    "\n",
    "    for cid, tfs in d_tf.items():\n",
    "        token_bm25_score[cid] = defaultdict(float)\n",
    "        for tid, tf in tfs.items():\n",
    "            token_bm25_score[cid][tid] = tf * (1 + k1) / (tf + k1 * (1 - b + b * doc_lens[cid] / doc_len_ave)) * idf[tid]\n",
    "            \n",
    "    return token_bm25_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860d305d-526f-4b02-8bfa-46cb4552074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebc7bc3-09fa-4555-9f8a-819b88244d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "hf_d_tf, hf_idf, hf_doc_lens, hf_doc_len_ave = get_info(corpus, hf_tokenizer.tokenize)\n",
    "hf_token_bm25_score = get_d_bm25(hf_d_tf, hf_idf, hf_doc_lens, hf_doc_len_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f67750-9281-4745-b3f4-d0b645353536",
   "metadata": {},
   "outputs": [],
   "source": [
    "del hf_d_tf, hf_doc_lens, hf_doc_len_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8140dba4-05a9-473a-9b39-768844b2e06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tokenizer = Analyzer(get_lucene_analyzer())\n",
    "lm_d_tf, lm_idf, lm_doc_lens, lm_doc_len_ave = get_info(corpus, lm_tokenizer.analyze)\n",
    "lm_token_bm25_score = get_d_bm25(lm_d_tf, lm_idf, lm_doc_lens, lm_doc_len_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956d4ec7-c80d-4933-9753-87198cc20f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "del lm_d_tf, lm_doc_lens, lm_doc_len_ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ee7a7c-fb29-41dc-aa6a-58f42345a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "with open(f\"./analysis_data/{dataset}/bm25_result.json\") as f:\n",
    "    bm25_result = json.load(f)\n",
    "    \n",
    "with open(f\"./analysis_data/{dataset}/dense_result.json\") as f:\n",
    "    dense_result = json.load(f)\n",
    "    \n",
    "with open(f\"./analysis_data/{dataset}/weighted_dense_result.json\") as f:\n",
    "    weighted_dense_result = json.load(f)\n",
    "    \n",
    "with open(f\"./analysis_data/{dataset}/cbm25_result.json\") as f:\n",
    "    cbm25_result = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de921403-7dbd-483b-8840-6dce6f475fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e97b0-7dfd-484d-b14a-ec6eb1e806ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_scores = evaluator.evaluate(bm25_result)\n",
    "dense_scores = evaluator.evaluate(dense_result)\n",
    "wdense_scores = evaluator.evaluate(weighted_dense_result)\n",
    "cbm25_scores = evaluator.evaluate(cbm25_result[func_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79037fe8-4b9a-4f98-bbac-04c1f4d53ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bm25: \", np.average([i[\"ndcg_cut_10\"] for i in bm25_scores.values()]))\n",
    "print(\"dense: \", np.average([i[\"ndcg_cut_10\"] for i in dense_scores.values()]))\n",
    "print(\"wdense: \", np.average([i[\"ndcg_cut_10\"] for i in wdense_scores.values()]))\n",
    "print(\"cbm25: \", np.average([i[\"ndcg_cut_10\"] for i in cbm25_scores.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecb94c-e63b-4a92-99a0-835bd0d3d71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_diff_scores(source_scores, target_scores):\n",
    "    diff_scores = {}\n",
    "    for k in cbm25_scores:\n",
    "        diff_score = source_scores[k][\"ndcg_cut_10\"] - target_scores[k][\"ndcg_cut_10\"]\n",
    "        if abs(diff_score) > 0.0:\n",
    "            diff_scores[k] = diff_score\n",
    "            \n",
    "    return diff_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a65db-7d8f-4f88-8c3b-5aa4e22e195b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_scores_bm25 = create_diff_scores(cbm25_scores, bm25_scores)\n",
    "diff_scores_dense = create_diff_scores(cbm25_scores, dense_scores)\n",
    "diff_scores_wdense = create_diff_scores(cbm25_scores, wdense_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855c2db-0917-4e80-b326-453601dcbc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_diff_query = set(diff_scores_bm25) & set(diff_scores_dense) & set(diff_scores_wdense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa13de-743e-46d8-b86c-57fe6330f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_diff = pd.concat([pd.Series(diff_scores_bm25), pd.Series(diff_scores_dense), pd.Series(diff_scores_wdense)], axis=1).fillna(0).sort_index()\n",
    "all_diff = all_diff.rename(columns={0: \"bm25\", 1: \"dense\", 2: \"wdense\"})\n",
    "all_diff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd06f0c-8826-41d0-bdb2-dbaf88eab914",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_query = all_diff[(all_diff > 0.2).all(1)]\n",
    "better_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f67d15a-9eb7-4a14-ab75-c5f3b846f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "worse_query = all_diff[(all_diff < -0.01).all(1)]\n",
    "worse_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a50b3b-f36d-4357-a83a-3eef1a3ea366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_top1(qid, show_condition):\n",
    "    def extract_top1(result):\n",
    "        return sorted(result[qid].items(), key=lambda x: -x[1])[0]\n",
    "    \n",
    "    bm25_top1 = extract_top1(bm25_result)\n",
    "    dense_top1 = extract_top1(dense_result)\n",
    "    wdense_top1 = extract_top1(weighted_dense_result)\n",
    "    cbm25_top1 = extract_top1(cbm25_result[func_name])\n",
    "    \n",
    "    correct = qrels[qid]\n",
    "    if show_condition(cbm25_top1[0], bm25_top1[0], correct):\n",
    "        print(\"------\")\n",
    "        print(f\"qid: {qid}, query: {queries[qid]}\")\n",
    "        print(bm25_top1[0] in correct, f\"bm25: {corpus[bm25_top1[0]]}\")\n",
    "        print(dense_top1[0] in correct, f\"dense: {corpus[dense_top1[0]]}\")\n",
    "        print(wdense_top1[0] in correct, f\"wdense: {corpus[wdense_top1[0]]}\")\n",
    "        print(cbm25_top1[0] in correct, f\"cbm25: {corpus[cbm25_top1[0]]}\")\n",
    "    else:\n",
    "        print(\"------\")\n",
    "        print(f\"skip qid {qid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60752ac3-5aef-40fa-9530-a24730d77252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_condition(target, compare, correct):\n",
    "    return target in correct and not compare in correct\n",
    "\n",
    "for qid in better_query.index:\n",
    "    show_result_top1(qid, correct_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3972a-507e-45b4-8572-8e236828f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_condition(target, compare, correct):\n",
    "    return not target in correct and compare in correct\n",
    "\n",
    "for i in worse_query.index:\n",
    "    show_result_top1(i, false_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076832f8-19d9-409a-9bbe-75f2b12567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_rep(reps: np.ndarray, att_mask: np.ndarray, input_tok: np.ndarray):\n",
    "    reps = rep_lave(reps, att_mask)\n",
    "    reps /= np.linalg.norm(reps, axis=2)[:, :, np.newaxis]\n",
    "    reps[np.isnan(reps)] = 0.0\n",
    "    return reps, att_mask[:, 1:], input_tok[:, 1:]\n",
    "\n",
    "def rep_lave(reps, att_masks, window_size=3):\n",
    "    tg_reps = np.zeros_like(reps[:, 1:])  # 3D\n",
    "    for b, (rep, att_mask) in enumerate(zip(reps, att_masks)):\n",
    "        og_rep = rep[att_mask == 1, :]  # 2D\n",
    "        og_rep = og_rep[1:-1, :]  # remove special token\n",
    "        rep_len = og_rep.shape[0]\n",
    "        for i in range(rep_len):\n",
    "            start = i - window_size if i - window_size > 0 else 0\n",
    "            end = i + window_size\n",
    "            tg_reps[b, i, :] += np.mean(og_rep[start:end, :], axis=0)\n",
    "\n",
    "    return tg_reps\n",
    "\n",
    "def max_cos_sims(query, doc, model, tokenizer):\n",
    "    special_tokens = {\n",
    "        tokenizer.pad_token_id,\n",
    "        tokenizer.bos_token_id,\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.sep_token_id,\n",
    "        tokenizer.cls_token_id,\n",
    "    }\n",
    "    def tok2rep_indexing(inputs_ids, batch_reps, att_masks):\n",
    "        tok2rep = defaultdict(list)\n",
    "        for qi, (input_ids, reps, att_mask) in enumerate(zip(inputs_ids, batch_reps, att_masks)):\n",
    "            for i, (qt, rep, am) in enumerate(zip(input_ids, reps, att_mask)):\n",
    "                if qt in special_tokens:\n",
    "                    continue\n",
    "                if am == 0:\n",
    "                    continue\n",
    "                tok2rep[qt].append(rep)\n",
    "            \n",
    "        for tid in tok2rep:\n",
    "            tok2rep[tid] = np.vstack(tok2rep[tid])\n",
    "            \n",
    "        return tok2rep\n",
    "        \n",
    "    doc = doc[\"title\"] + \" \" + doc[\"text\"]\n",
    "    t_query = tokenizer(query, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        e_query = model(**t_query).last_hidden_state\n",
    "    e_queries, q_att_masks, q_inputs_ids = preproc_rep(e_query.numpy(), t_query[\"attention_mask\"].numpy(), t_query[\"input_ids\"].numpy())\n",
    "    q_tok2rep = tok2rep_indexing(q_inputs_ids, e_queries, q_att_masks)\n",
    "    t_doc = tokenizer(doc, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        e_doc = model(**t_doc).last_hidden_state\n",
    "    e_docs, d_att_masks, d_inputs_ids = preproc_rep(e_doc.numpy(), t_doc[\"attention_mask\"].numpy(), t_doc[\"input_ids\"].numpy())\n",
    "    # print(e_docs.shape, d_att_masks.shape, d_inputs_ids.shape, tokenizer.convert_ids_to_tokens(list(d_inputs_ids[0])))\n",
    "    d_tok2rep = tok2rep_indexing(d_inputs_ids, e_docs, d_att_masks)\n",
    "    result = []\n",
    "    for qt, q_reps in q_tok2rep.items():\n",
    "        q_token = tokenizer.convert_ids_to_tokens(int(qt))\n",
    "        if qt not in d_tok2rep:\n",
    "            result.append(f\"{q_token}: 0.0\")\n",
    "            continue\n",
    "            \n",
    "        score=np.max(np.dot(q_reps, d_tok2rep[qt].T))\n",
    "        result.append(f\"{q_token}: {round(float(score), 2)}\")\n",
    "        \n",
    "    return \", \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac694da-5111-422b-a367-b39a4251fcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_doc(target, tokenizer):\n",
    "    text = corpus[target][\"title\"] + \" \" + corpus[target][\"text\"]\n",
    "    return tokenizer(text)\n",
    "    \n",
    "def get_bm25_val(cid, query, token_bm25_score, tokenizer):\n",
    "    this_q_tok_bm25 = []\n",
    "    t_query = tokenizer(query)\n",
    "    for tok in t_query:\n",
    "        this_q_tok_bm25.append(f\"{tok}: {round(token_bm25_score[cid][(tok)], 2)}\")\n",
    "    return \", \".join(this_q_tok_bm25)\n",
    "\n",
    "def get_idf_val(query, idf, tokenizer):\n",
    "    this_q_tok_idf = []\n",
    "    t_query = tokenizer(query)\n",
    "    for tok in t_query:\n",
    "        this_q_tok_idf.append(f\"{tok}: {round(idf[tok], 2)}\")\n",
    "    return \", \".join(this_q_tok_idf)\n",
    "        \n",
    "\n",
    "def show_result_top1_analysis(qid, show_condition):\n",
    "    def extract_top1(result):\n",
    "        return sorted(result[qid].items(), key=lambda x: -x[1])[0]\n",
    "    \n",
    "    bm25_top1 = extract_top1(bm25_result)\n",
    "    dense_top1 = extract_top1(dense_result)\n",
    "    wdense_top1 = extract_top1(weighted_dense_result)\n",
    "    cbm25_top1 = extract_top1(cbm25_result[func_name])\n",
    "    correct = qrels[qid]\n",
    "    query = queries[qid]\n",
    "    if show_condition(cbm25_top1[0], bm25_top1[0], correct):\n",
    "        print(\"------\")\n",
    "        print(f\"qid: {qid}, query: {query}\")\n",
    "        print(f\"lm_idf: {get_idf_val(query, lm_idf, lm_tokenizer.analyze)}\")\n",
    "        print(f\"hf_idf: {get_idf_val(query, hf_idf, hf_tokenizer.tokenize)}\")\n",
    "        print(\"---\")\n",
    "        cid_bm25_top1 = bm25_top1[0]\n",
    "        cid_bm25_tok_score = get_bm25_val(cid_bm25_top1, query, lm_token_bm25_score, lm_tokenizer.analyze)\n",
    "        t_doc = tokenizer_doc(cid_bm25_top1, lm_tokenizer.analyze)\n",
    "        print(f\"bm25: {cid_bm25_top1 in correct} q-bm25: {cid_bm25_tok_score}\")\n",
    "        print(f\"t_doc: {t_doc}\")\n",
    "        cid_dense_top1 = dense_top1[0]\n",
    "        cid_dense_tok_score = get_bm25_val(cid_dense_top1, query, hf_token_bm25_score, hf_tokenizer.tokenize)\n",
    "        t_doc = tokenizer_doc(cid_dense_top1, hf_tokenizer.tokenize)\n",
    "        print(f\"dense: {cid_bm25_top1 in correct} q-bm25: {cid_dense_tok_score}\")\n",
    "        print(f\"t_doc: {t_doc}\")  \n",
    "        cid_wdense_top1 = wdense_top1[0]\n",
    "        cid_wdense_tok_score = get_bm25_val(cid_wdense_top1, query, hf_token_bm25_score, hf_tokenizer.tokenize)\n",
    "        t_doc = tokenizer_doc(cid_wdense_top1, hf_tokenizer.tokenize)\n",
    "        print(f\"wdense: {cid_bm25_top1 in correct}, q-bm25: {cid_wdense_tok_score}\")\n",
    "        print(f\"t_doc: {t_doc}\")  \n",
    "        cid_cbm25_top1 = cbm25_top1[0]\n",
    "        cid_cbm25_tok_score = get_bm25_val(cid_cbm25_top1, query, hf_token_bm25_score, hf_tokenizer.tokenize)\n",
    "        t_doc = tokenizer_doc(cid_cbm25_top1, hf_tokenizer.tokenize)\n",
    "        t_cos_sims = max_cos_sims(query, corpus[cid_cbm25_top1], hf_model, hf_tokenizer)\n",
    "        print(f\"cbm25: {cid_cbm25_top1 in correct} q-bm25: {cid_cbm25_tok_score}\")\n",
    "        print(f\"cos-sims: {t_cos_sims}\")\n",
    "        print(f\"t_doc: {t_doc}\")\n",
    "    else:\n",
    "        print(\"------\")\n",
    "        print(f\"skip qid {qid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c5388-aca1-4d71-9055-e80ba29c756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid in better_query.index:\n",
    "    show_result_top1_analysis(qid, correct_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d7ab3-ecd5-4d08-89ac-91d6b689dcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid in worse_query.index:\n",
    "    show_result_top1_analysis(qid, false_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88699d70-e107-4e5f-9e67-9cfdbdd1b4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cbm25_result[func_name][\"INEX_XER-141\"].items(), key=lambda x: -x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd449d8e-5101-4e66-b894-9db74e94fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = \"QALD2_te-28\"\n",
    "cid = \"<dbpedia:Don't_Box_Me_In>\"\n",
    "query = queries[qid]\n",
    "print(query)\n",
    "print(\"bm25\")\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))\n",
    "print(\"cmb25\")\n",
    "cid = \"<dbpedia:The_Godfather>\"\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7148c-121e-4ea4-95d5-2ff02984cff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = \"INEX_LD-20120512\"\n",
    "cid = '<dbpedia:GP_Basic>'\n",
    "query = queries[qid]\n",
    "print(query)\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a8966-3ffe-4dfc-9968-5e03bb110302",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = \"INEX_XER-141\"\n",
    "cid = '<dbpedia:Open_University_of_Catalonia>'\n",
    "query = queries[qid]\n",
    "print('dense')\n",
    "print(query)\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))\n",
    "print(\"cbm25\")\n",
    "cid = '<dbpedia:CatalunyaCaixa>'\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6e03f-6a58-4104-b31e-15499c994d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = \"INEX_LD-20120121\"\n",
    "cid = \"<dbpedia:Raw_Food_Made_Easy_for_1_or_2_People>\"\n",
    "query = queries[qid]\n",
    "print(query)\n",
    "print(\"bm25\")\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))\n",
    "print(tokenizer_doc(cid, hf_tokenizer.tokenize))\n",
    "print(\"cbm25\")\n",
    "cid = \"<dbpedia:Luke_Nguyen's_Vietnam>\"\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbcc45-cfb3-48e7-9f78-c348c0c137c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sorted(cbm25_result[func_name][\"INEX_LD-20120121\"].items(), key=lambda x: -x[1]):\n",
    "    print(i, corpus[i[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31566f45-96b6-47e5-b0f5-63f0569328e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_tokenizer.analyze(\"Vietnamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646bf9f-2e5f-4c82-9ee1-ffbca191be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_top1_comp_wdense(qid, show_condition):\n",
    "    def extract_top1(result):\n",
    "        return sorted(result[qid].items(), key=lambda x: -x[1])[0]\n",
    "    \n",
    "    bm25_top1 = extract_top1(bm25_result)\n",
    "    dense_top1 = extract_top1(dense_result)\n",
    "    wdense_top1 = extract_top1(weighted_dense_result)\n",
    "    cbm25_top1 = extract_top1(cbm25_result[func_name])\n",
    "    \n",
    "    correct = qrels[qid]\n",
    "    # if show_condition(cbm25_top1[0], wdense_top1[0], correct) and wdense_top1[0] != dense_top1[0]:\n",
    "    if wdense_top1[0] != dense_top1[0]:\n",
    "        print(\"------\")\n",
    "        print(f\"qid: {qid}, query: {queries[qid]}\")\n",
    "        print(bm25_top1[0] in correct, f\"bm25: {corpus[bm25_top1[0]]}\")\n",
    "        print(dense_top1[0] in correct, f\"dense: {corpus[dense_top1[0]]}\")\n",
    "        print(wdense_top1[0] in correct, f\"wdense: {corpus[wdense_top1[0]]}\")\n",
    "        print(cbm25_top1[0] in correct, f\"cbm25: {corpus[cbm25_top1[0]]}\")\n",
    "    else:\n",
    "        print(\"------\")\n",
    "        print(f\"skip qid {qid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f34095-b0c6-429f-b538-f87e0b6334bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_qid = []\n",
    "for qid, sup in diff_scores_wdense.items():\n",
    "    if sup < 0:\n",
    "        continue\n",
    "    diff = wdense_scores[qid][\"ndcg_cut_10\"] - dense_scores[qid][\"ndcg_cut_10\"]\n",
    "    if diff > 0:\n",
    "        target_qid.append(qid)\n",
    "        \n",
    "for qid in target_qid:\n",
    "    show_result_top1_comp_wdense(qid, correct_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a0c9eb-0392-49eb-8bc0-a34ed57124e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pd.Series(pd.Series(diff_scores_wdense) > 0).index:\n",
    "    show_result_top1_comp_wdense(i, correct_condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2922e3c-d796-4b5d-ad1b-c468e6af4048",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cbm25_result[func_name][\"QALD2_te-28\"].items(), key=lambda x: -x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef28af7-338f-4d2a-a14f-f17720928777",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token_bm25_score['<dbpedia:All_of_Me_(1984_film)>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4304ff-56ed-4e38-a059-54ce1e18ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token_bm25_score['<dbpedia:The_Godfather>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3515851-0657-43db-9d70-2f6f97d96fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = \"QALD2_te-64\"\n",
    "cid = \"<dbpedia:Launch_Control_Center>\"\n",
    "query = queries[qid]\n",
    "print(query)\n",
    "print(\"bm25\")\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))\n",
    "print(\"cmb25\")\n",
    "cid = \"<dbpedia:Vandenberg_AFB_Space_Launch_Complex_4>\"\n",
    "print(get_bm25_val(cid, query, hf_token_bm25_score, hf_tokenizer.tokenize))\n",
    "print(max_cos_sims(query, corpus[cid], hf_model, hf_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c79576-581a-4d9e-a0af-6309feaa5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token_bm25_score[\"<dbpedia:Launch_Control_Center>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb4ffac-e050-4243-85fa-ad10d5a08d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token_bm25_score[\"<dbpedia:Vandenberg_AFB_Space_Launch_Complex_4>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa23b8-6247-453a-b320-71ebdbe38ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Carl Reiner\", \"Steve Martin\", \"Lily Tomlin \"]\n",
    "for name in names:\n",
    "    print(\", \".join([f\"{tn}: {round(hf_idf[tn], 2)}\" for tn in hf_tokenizer.tokenize(name)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9e74f-574a-40de-8c50-b43235e73154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
